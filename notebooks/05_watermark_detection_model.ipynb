{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e929c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label AI and Watermarked AI\n",
    "import pandas as pd\n",
    "\n",
    "df_clean = pd.read_csv(\"../data/ai_samples.csv\")\n",
    "df_clean[\"Label\"] = \"AI_Plain\"\n",
    "\n",
    "df_watermarked = pd.read_csv(\"../data/ai_samples_watermarked.csv\")\n",
    "df_watermarked[\"Label\"] = \"AI_Watermarked\"\n",
    "\n",
    "df_combined = pd.concat([df_clean, df_watermarked]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75dc4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                                              Question  \\\n",
       "0   How do you feel about Elon Musk saying that Tr...   \n",
       "1   How do you feel about Elon Musk saying that Tr...   \n",
       "2   How do you feel about Elon Musk saying that Tr...   \n",
       "3   How do you feel about Elon Musk saying that Tr...   \n",
       "4   How do you feel about Elon Musk saying that Tr...   \n",
       "..                                                ...   \n",
       "95                   The power of vulnerability | TED   \n",
       "96                   The power of vulnerability | TED   \n",
       "97  The Unstoppable Power of Letting Go | TEDxWilm...   \n",
       "98  The Unstoppable Power of Letting Go | TEDxWilm...   \n",
       "99  The Unstoppable Power of Letting Go | TEDxWilm...   \n",
       "\n",
       "                                                 Text            Source  \\\n",
       "0   Honestly, it’s kind of surreal how casually so...       r/AskReddit   \n",
       "1   Part of me thinks Elon just likes chaos. He kn...       r/AskReddit   \n",
       "2   Elon Musk saying Trump is in the Epstein files...       r/AskReddit   \n",
       "3   It feels like a strategic distraction move, if...       r/AskReddit   \n",
       "4   I'm not shocked if it's true, but I am shocked...       r/AskReddit   \n",
       "..                                                ...               ...   \n",
       "95  This talk changed how I show up in my relation...  Youtube Comments   \n",
       "96  I didn’t realize how much I was numbing my emo...  Youtube Comments   \n",
       "97  Letting go used to sound like giving up to me....  Youtube Comments   \n",
       "98  Wow, this was powerful. I’ve been holding onto...  Youtube Comments   \n",
       "99  It’s wild how something so simple — just letti...  Youtube Comments   \n",
       "\n",
       "             Label                                   Text_watermarked  \n",
       "0         AI_Plain                                                NaN  \n",
       "1         AI_Plain                                                NaN  \n",
       "2         AI_Plain                                                NaN  \n",
       "3         AI_Plain                                                NaN  \n",
       "4         AI_Plain                                                NaN  \n",
       "..             ...                                                ...  \n",
       "95  AI_Watermarked  This talk changed how ​I show up in my relatio...  \n",
       "96  AI_Watermarked  I ​didn’t realize how much I was numbing my em...  \n",
       "97  AI_Watermarked  Letting go used to sound like giving up to me。...  \n",
       "98  AI_Watermarked  Wow، this was powerful。 It turns out، I’ve bee...  \n",
       "99  AI_Watermarked  It’s wild how ​something so simple — just lett...  \n",
       "\n",
       "[100 rows x 5 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcaf93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # for tokenizing text into words and sentences\n",
    "import string # for getting list of punctuations\n",
    "\n",
    "#nltk.download('punkt_tab') # model by nltk for word and sentence tokenization\n",
    "\n",
    "# take a block of text and calculate features related to writing style\n",
    "def extract_features(text):\n",
    "\n",
    "    # split into sentences and words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "\n",
    "    avg_word_length = sum(len(w) for w in words)/ word_count if word_count> 0 else 0\n",
    "    punctuation_count = sum(1 for c in text if c in string.punctuation)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"punctuation_count\": punctuation_count\n",
    "    })\n",
    "\n",
    "df_features = df_combined[\"Text\"].apply(extract_features)\n",
    "df_combined = pd.concat([df_combined,df_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a736005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stylometric and TF ID features again\n",
    "\n",
    "def contains_zwsp(text):\n",
    "    return \"\\u200b\" in text\n",
    "\n",
    "\n",
    "def contains_arabic_comma(text):\n",
    "    return \"،\" in text\n",
    "\n",
    "df_combined[\"has_zwsp\"] = df_combined[\"Text\"].apply(contains_zwsp).astype(int)\n",
    "df_combined[\"has_arabic_comma\"] = df_combined[\"Text\"].apply(contains_arabic_comma).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af74b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# 1. TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=300, stop_words=\"english\")\n",
    "X_tfidf = vectorizer.fit_transform(df_combined[\"Text\"])\n",
    "\n",
    "# 2. Stylometric features\n",
    "stylometric = df_combined[[\"word_count\", \"sentence_count\", \"avg_word_length\", \"punctuation_count\"]]\n",
    "stylometric_scaled = StandardScaler().fit_transform(stylometric)\n",
    "\n",
    "# 3. Binary watermark flags\n",
    "watermark_flags = df_combined[[\"has_zwsp\", \"has_arabic_comma\"]].values\n",
    "\n",
    "# Combine everything into X_combined\n",
    "X_combined = hstack([\n",
    "    X_tfidf,\n",
    "    csr_matrix(stylometric_scaled),\n",
    "    csr_matrix(watermark_flags)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8a2870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                has_zwsp  has_arabic_comma\n",
      "Label                                     \n",
      "AI_Plain             0.0               0.0\n",
      "AI_Watermarked       0.0               0.0\n"
     ]
    }
   ],
   "source": [
    "print(df_combined.groupby('Label')[['has_zwsp', 'has_arabic_comma']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3a3191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      AI_Plain       0.09      0.10      0.10        10\n",
      "AI_Watermarked       0.00      0.00      0.00        10\n",
      "\n",
      "      accuracy                           0.05        20\n",
      "     macro avg       0.05      0.05      0.05        20\n",
      "  weighted avg       0.05      0.05      0.05        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# X = TF-IDF + stylometrics + binary Unicode features\n",
    "# y = AI_Plain vs AI_Watermarked\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, df_combined[\"Label\"], stratify=df_combined[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "218f2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X_combined, df_combined[\"Label\"], cv=5)\n",
    "print(f\"Cross-val accuracy: {scores.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d89228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      AI_Plain       0.62      0.60      0.61        40\n",
      "AI_Watermarked       0.61      0.62      0.62        40\n",
      "\n",
      "      accuracy                           0.61        80\n",
      "     macro avg       0.61      0.61      0.61        80\n",
      "  weighted avg       0.61      0.61      0.61        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "print(classification_report(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c887d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      AI_Plain       0.09      0.10      0.10        10\n",
      "AI_Watermarked       0.00      0.00      0.00        10\n",
      "\n",
      "      accuracy                           0.05        20\n",
      "     macro avg       0.05      0.05      0.05        20\n",
      "  weighted avg       0.05      0.05      0.05        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00cc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
